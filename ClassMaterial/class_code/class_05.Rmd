---
title: "Class 5 notes and code"
output:
  pdf_document: default
  html_document: default
---



$\\$




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)

knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```





<!--  Please run the code in the R chunk below once. It will install a packages, and and download figures/data we will use.  -->

```{r, eval = FALSE}


# makes sure you have all the packages we have used in class installed
SDS230::update_installed_packages()

# get some images that are used in this document
SDS230::download_image("area_pdf.png")
SDS230::download_image("normal_pillow.jpg")
SDS230::download_image("normal_distrubution_3sds.png")

```





$\\$




## Overview

 * Quick review of probability distributions
 * Sampling distributions 
 * Confidence intervals
 * The bootstrap
 * Using the bootstrap to calculate confidence intervals in R



$\\$



## Part 1: Review


$\\$




#### Part 1.1: Probability density functions 


$\\$


Probability density functions can be used to model random events. All **probability density functions**, *f(x)*, have these properties:

1. The function are always non-negative
2. The area under the function integrates (sums) to 1


For continuous (quantitative) data, we use density function f(x) to find the probability (e.g., model of the long run frequency) that a random outcome X is between two values *a* and *b* using:


$P(a < X < b) = \int_{a}^{b}f(x)dx$


![](area_pdf.png)




Probability density functions are a way to describe the (potentially infinite) population data (think Plato). Population density functions typically have *parameters* which we can estimate from by calculating *statistics* from samples of data.

Note: the term **probability distribution** is a term that can refer to:

1. the *probability density function* (or the probability mass function for discrete data)
2. the *cumulative distribution function* which is the integral (sum) of the density function

$$P(X < x) = F(x) = \int_{y < x}f(y)dy$$



$\\$




##### Example 1: The normal distribution 

The normal distribution has two parameters that define the distribution:

  1. $\mu$: which is the center of the distribution
  2. $\sigma$: which is the spread of the distribution 
  
  
The equation for the normal density function is:

$f(x ; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-\frac{(x - \mu)^2}{2 \sigma^2}}$


A given set of values $\mu$ and $\sigma$ define a specific normal distribution, from the family of all possible normal distributions (i.e, all possible values for $\mu$ and $\sigma$).

Below we plot a normal distribution for $\mu$ = 100 and $\sigma$ = 15


```{r normal_density}

x <- seq(30, 180, by = .001)  # a range of x values
y <- dnorm(x, 100, 15)  # the density function at each value of x

# plot the probability density function 
plot(x, y, 
     type = 'l', 
     ylab = "f(x)", 
     main = TeX("Normal density with $\\mu$ = 100, $\\sigma$ = 15"))


# we can get random points from this distribution using the rnorm function
n <- 20
(rand_data <- rnorm(n, 100, 15))

```


Important facts about the normal distribution: 
 
 * 68% of the probability mass is within 1 $\sigma$ of $\mu$
 * 95% of the probability mass is within 2 $\sigma$ of $\mu$
 * 99.7% of the probability mass is within 3 $\sigma$ of $\mu$


![](normal_distrubution_3sds.png)



![](normal_pillow.jpg)


$\\$




##### Example 2: The exponential distribution

The exponential distribution has one parameter that defines the distribution: 

  1. $\lambda$: the rate parameter  ($\lambda > 0$)
  
  
The equation for the exponential density function is:

$f(x; \lambda) = \lambda e^{-\lambda x}$, for $x \ge 0$, and 0 otherwise.


You will explore this distribution more and sampling distributions derived from it on the homework.  


*Bonus exercise*: Use calculus to show this is a valid probability density function (i.e, it is non-negative and integrates to 1). 




$\\$




#### Part 1.2: The law of large numbers




$\\$



The *law of large numbers* states that as the sample size *n* grows the sample average approaches the expected value (i.e., the parameter $\mu$). We can write this as $n \rightarrow \infty$  $\bar{x} \rightarrow \mu$. 


This is a nice theoretical property that our statistic converges to our parameter value (i.e., that our statistic is consistent), but we live in the *real world* where we have finite data. What is of real interest is if we have a finite sample size *n* what does the *distribution of statistics* we get look like, i.e., what does the **sampling distribution** look like? 




$\\$







## Part 2: Sampling distributions and the standard error



#### Part 2.1: Sampling distributions


A distribution of statistics is called a **sampling distribution**. 

Let's use simulations to generate an approximation to a sampling distribution (a true sampling distribution would have all possible statistic values in it). 


```{r}


# create a sampling distribution of the mean using data from a uniform distribution
sampling_dist <- NULL



# plot a histogram of the sampling distribution of these means 




```


For many statistics, and the sample average $\bar{x}$ in particular, our sampling distribution is normal. Any ideas why this is? 

We will discuss ways to visualize whether data comes from a particular distribution, later in the semester. 



$\\$





#### Part 2.2: The standard error (SE)

The deviation of a sampling distribution is called the standard error (SE). Can you calculate the standard error for the sampling distribution you created above? 

```{r}

# caclulate the SE as the standard deviation of the sampling distribution



```






$\\$






## Part 3:  The central limit theorem

The [central limit theorm (CTL)](https://en.wikipedia.org/wiki/Central_limit_theorem) establishes that (in most situations) when independent random variables are added their (normalized) **sum** converges to a normal distribution.

Put another way, if we define the average random (i.i.d) sample {$X_1$, $X_2$, ..., $X_n$} of size *n* as: 

$S_{n}:={\frac{X_{1}+\cdots +X_{n}}{n}}$

then the CTL tells us that:

$\sqrt{n}(S_{n} - \mu)$ $\xrightarrow {d} N(0,\sigma^{2})$


You will explore this more through simulations on homework 2. 





$\\$





## Part 4: Confidence intervals


$\\$


Confidence intervals are ranges of values that capture a parameter a fix proportion of time.

Let's explore this more through the class slides...



$\\$






## Part 5: The bootstrap

The bootstrap is a method that can be used to estimate confidence intervals for a large range of parameters. 

The central concept behind the bootstrap is the "plug-in principle" where we treat our sample of data as if it were the population. We then sample *with replacement* from **our sample** to create a *bootstrap distribution* which is a proxy for (the spread of) the sampling distribution. 



$\\$



#### Part 5.1: Creating a bootstrap distribution in R


To sample data in R we can use the `sample(the_data)` function. To sample data with replacement we use the `replace = TRUE` argument, i.e., `sample(the_data, replace = TRUE)`.  

Below we calculate the bootstrap distribution for mean age of OkCupid users using just the first 20 OkCupid users in the data set. 


```{r}

library(okcupiddata)

# get the ages from the first 20 OkCupid profiles


# create the bootstrap distribution



# plot the bootstrap distribution to make sure it looks normal



```



$\\$




#### Part 5.2: Calculating the bootstrap standard error SE*


The standard deviation of the *bootstrap distribution* is usually a good approximation of the standard deviation of the sampling distribution - i.e, it is a good approximation of the *standard error SE*.

When our bootstrap distribution is relatively normal, we can use the fact that 95% of values fall within to standard deviations of a normal distribution to calculate 95% confidence intervals as:

$CI_{95} = [stat - 2 \cdot SE^*,  stat + 2 \cdot SE^*]$

For example, for a our bootstrap distribution we have a 95% confidence interval for the mean $\mu$ as: 

$CI_{95} = [\bar{x} - 2 \cdot SE^*,  \bar{x} + 2 \cdot SE^*]$




```{r bootstrap_SE}

# calculate the bootstrap standard error SE* as the standard deviation of the bootstrap distribution 


# calculate the 95% CI using SE*





```



Above we are using the bootstrap to create a 95% confidence interval which should capture the mean age $\mu_{age}$








