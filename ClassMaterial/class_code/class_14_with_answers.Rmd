---
title: "Class 14 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$







<!--  Please run the code in the  R chunk below once. This will install some packages and download data and images needed for these exercises.  -->

```{r message=FALSE, warning = FALSE, echo = FALSE, eval = FALSE}

# makes sure you have all the packages we have used in class installed
# SDS230::update_installed_packages()


SDS230::download_image("qq-plot.png")

SDS230::download_data("amazon.rda")

```





```{r setup, include=FALSE}


library(latex2exp)

options(scipen=999)
knitr::opts_chunk$set(echo = TRUE)

set.seed(230)

```





$\\$







## Overview

 * Visual hypothesis tests
 * Quantile-quantile plots
 * Functions in R and conditional statements
 * Writing a function to create bootstrap confidence intervals and evaluating confidence interval coverage



$\\$


## Part 1: Visual hypothesis tests

In "visual hypothesis tests", we assess whether we can identify which plot contains relationships in the data, from plots that show scrambled versions of the data. For more information about these tests, and an R package that implements these tests, see:

- https://cran.r-project.org/web/packages/nullabor/vignettes/nullabor.html
- https://vita.had.co.nz/papers/inference-infovis.pdf



$\\$




## Part 1.1: An example problem

Let's run a visual hypothesis test to see if there is a correlation between the number of pages in a book and the
price. We can start by stating the null and alternative hypothesis:

**In words:** 

*Null hypothesis*: there is no relationship between the  number of pages in a book and the price.

*Alternative hypothesis*: books that have more pages have higher prices

**In symbols:**

$H_0: \rho = 0$

$H_0: \rho > 0$



Let's now run the visual hypothesis test. Where we have:

- The "observed visualization" (step 2) is the plot of the actual data.

- The "null visualizations" (step 3) are visualizations of data where the relationship between weight and mpg has been shuffled.

- The "p-value/decision" (step 4-5) is whether we can identify the real plot from the shuffled plots.




$\\$




## Part 1.2: Running the analysis


Let's use the `nullabor` package to run the analysis. We will use the `null_permute()` function to randomly shuffle the `List.Price` variable and we will use the `lineup()` to create a data frame that has the real data and 19 other randomly shuffled data sets. We can then plot the real and shuffled data sets using `ggplot()` to see if we can identify the real one. We can also use the `decrypt()` function to reveal the answer about which data set is the real one. 



```{r visual_hypo_test}


# install.packages("nullabor")

library(nullabor)
library(ggplot2)

load("amazon.rda")


d <- lineup(null_permute("List.Price"), amazon)

ggplot(data=d, aes(x=NumPages, y=List.Price)) + 
  geom_point() + 
  facet_wrap(~ .sample)


```



The `nullabor` package can be used to examine other types of relationships, including examining differences in word useage (word clouds) and assessing whether there are trends on a map. 




$\\$






## Part 2: Examining how data is distributed using quantile-quantile plots


We can check if empirical data seems to come from a particular distribution using quantile-quantile plots (qqplots). qqplots plot the sorted data values in your sample as a function of the theoretical quantile values (at evenly spaced probability areas).

Below is an illustration of quantile values for 10 data points that come from a normal distribution. If we have 10 data points in our sample, then to create a qqplot comparing our data to a standard normal distribution we would plot our data as a function of these theoretical quantile values. If the plot falls along a diagonal line, this indicates our data comes from the standard normal distribution 

Also see [this explanation](https://www.statisticshowto.datasciencecentral.com/q-q-plots/)

![](qq-plot.png)



$\\$




### Part 2.1


Let's create a qqplot to assess whether the thickness of books on Amazon are normally distributed.



```{r qqplots}


# get the heights of all baseball players
load("amazon.rda")
thickness <- amazon$Thick



# view a histogram of book thicknesses
hist(thickness, breaks = 20)


# create an sequence of values between 0 and 1 at even spaces
prob_area_vals <- seq(0, 1, length.out = length(thickness))


# get the quantiles from these values
quantile_vals <- qnorm(prob_area_vals)


# create the qqplot
plot(quantile_vals, sort(thickness),
     xlab = "Normal quantiles",
     ylab = "Data quantiles",
     main = "Quantile-quantile plot")

```



$\\$



### Part 2.2



We can also use the `qqnorm()` function to do this more easily when comparing data to the normal distribution. Or, if we want a better visualization, we can use the `qqPlot()` function in the `car` package.


```{r qqnorm}

qqnorm(thickness)

#install.packages("car")
car::qqPlot(thickness)

```



This data is pretty normal as can see in the plots above. Let's look at some highly skewed data.



```{r skewed_qqplot}



# data that is skewed to the right
exp_data <- rexp(1000)

hist(exp_data, breaks = 50, 
     main = "Data from an exponential distribution", 
     xlab = "Data values")

qqnorm(exp_data)



# data that is skewed to the left
exp_data <- -1 * rexp(1000)

hist(exp_data, breaks = 50, 
     main = "Left skewed data", 
     xlab = "Data values")

qqnorm(exp_data)



```







$\\$



## Part 3: Writing functions in R and conditional statements




### Part 3.1: Writing functions 


We can write functions in R using the `function()` function!

Let's write a function called `cube_it()` that takes a value `x` and returns `x` cubed.



```{r writing_functions}


# the square root function
sqrt(49)



# a function that takes a value to the third power
cube_it <- function(x) {
  
  x^3

}


cube_it(2)


```






$\\$





### Part 3.2: Addtional function arguments


We can specify additional arguments which can also have default values. 

Let's write a function called `power_it` which takes a value `x` and a value `pow` and returns `x` to the pow power. Let's also have the default value of the `pow` argument be 3.


```{r writing_functions2}


# a function that takes a value to the third power
power_it <- function(x, pow = 3) {
  
  x^pow

}



power_it(2)

power_it(2, 8)


```





$\\$



### Part 3.3: Conditional statements


We can use "if statements" to execute code that meets particular conditions.

Let's create an additional Boolean arugment called `verbose` that prints a message if the value is set to `TRUE`.


```{r writing_functions3}


power_and_like_it <- function(x, pow = 3, verbose = FALSE) {

  val <- x^pow

  if (verbose) {
    print(paste("I like the number", val))
  }
  
  
  val
  
}



result <- power_and_like_it(2)

result <- power_and_like_it(3, verbose = TRUE)



```





$\\$






### Part 3.4: Conditional statement with AND (&) and OR (|)

The AND (&) and OR (|) operators are used to connect two Boolean expressions: 

* The AND (&) returns TRUE if both expressions are TRUE.
* The | operator  returns TRUE if either expressions are TRUE.



```{r conditionals}

# Exploring the AND (&) operator

TRUE & TRUE
TRUE & FALSE


# Exploring the OR (|) operator

TRUE | TRUE
TRUE | FALSE
FALSE | FALSE

```



$\\$





## Part 4: Evaluating coverage of bootstrap confidence invervals  




### Part 4.1: Writing a function to create bootstrap confidence intervals for the mean

Let's write a function called `create_mean_bootstrap_CI()` that create bootstrap confidence intervals for the mean. The function should take the following arguments:

- `data_sample`: a sample of data
- `CI_level`: the confidence interval level with a default value of .95
- `num_boot_replicates`: the number of bootstrap statistics to have in our bootstrap distribution
- `plot_dist`: a Boolean specifying if we should plot the bootstrap distribution as a histogram


```{r CI_boot_function}
  

create_mean_bootstrap_CI <- function(data_sample, 
                                     CI_level = .95, 
                                     num_boot_replicates = 10000, 
                                     plot_dist = TRUE) {
  
  # create the bootstrap distribution
  boot_dist <- NULL
  for (i in 1:num_boot_replicates) {
    boot_sample <- sample(data_sample, replace = TRUE)
    boot_dist[i] <- mean(boot_sample)
  }
  
  
  # plot it
  if (plot_dist) {
    hist(boot_dist, breaks = num_boot_replicates/100,
         main = "Bootstrap distribution",
         xlab = TeX("$\\bar{x}^*"))
  }
  
  
  # calculate the bootstrap confidence interval
  tail_level <- (1 - CI_level)/2
  
  # we could use the quantile method to get confidence intervals
  # boot_CI <- quantile(boot_dist, c(tail_level, 1 - tail_level))
  
  boot_CI <- mean(data_sample) + qnorm(1 - tail_level) * sd(boot_dist) * c(-1, 1)
  
  
  boot_CI
  
  
} # end of the function


```



$\\$






### Part 4.2: Testing our bootstrap confidence interval function 


Let's test our bootstrap confidence interval function by creating a 95% confidence for the mpg cars in 1977 had (although the mtcars data set is not a random sample, so we need to interpret our results with a lot of caution). 



```{r test_boot_function}

mpg <- mtcars$mpg


create_mean_bootstrap_CI(mpg)


```




$\\$






### Part 4.3: Assessing bootstrap confidence interval coverage


As we know, for a 95% confidence level, 95% of our confidence intervals should contain the parameter of interest. Let's assess whether the bootstrap is really capturing the parameter 95% of the time using simulated data using a method that is similar to homework 4 problem 4.2. 


```{r boot_coverage, tidy=TRUE}
  

param_val <- .5

num_simultations <- 100

param_in_CI <- NULL
for (i in 1:num_simultations) {
  
  a_sample <- rexp(30, 1/param_val)   #runif(30)
  curr_CI <- create_mean_bootstrap_CI(a_sample, plot_dist = FALSE)
  
  param_in_CI[i] <- (curr_CI[1] <= param_val) & (curr_CI[2] >= param_val)
  
}


(prop_containing_parameter <- sum(param_in_CI)/num_simultations)

      
```


**Answer**: `r prop_containing_parameter * 100` percent of the confidence intervals that were created contained the parameter. This is the proportion I would expect from a 95% confidence interval estimator. 










