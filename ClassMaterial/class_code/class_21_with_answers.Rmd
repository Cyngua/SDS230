---
title: "Class 21 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$



<!--  Please run the code in the  R chunk below once. This will install some packages and download data and images needed for these exercises.  -->

```{r message=FALSE, warning = FALSE, echo = FALSE, eval = FALSE}

# makes sure you have all the packages we have used in class installed
SDS230::update_installed_packages()


```




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)
library(dplyr)
library(ggplot2)

#options(scipen=999)


knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```



$\\$




## Overview

 * Comparing models with R^2, adjusted R^2, AIC and BIC
 * Cross-validation



$\\$





## Comparing models with R^2, adjusted R^2, AIC, BIC and cross-validation


As we discussed, we can compare models using several measures including $R^2$, $R^2_{adj}$, $AIC$, $BIC$ and cross-validation. Let's try this out on the baseball data.




$\\$





#### Part 1.1: Comparing models with R^2, adjusted R^2, AIC, BIC


Let's compare our predictions of runs in baseball using the model with the original variables and the model that also includes the variables that includes statistics normalized by at bats. As we saw in the class videos, the variables that contain values normalized by at bats are redundant, so we might prefer models that do not contain these redundant variables. Let's see how our different statistics that assess model fit rank models that contain the redundant variables vs. models that do not contain these redundant variables.

The statistics we will look at are: 

 * R^2: higher means a better fit
 * Adjusted R^2: higher means a better fit
 * AIC: lower means a better model
 * BIC: lower means a better model


```{r}


library(Lahman)
library(dplyr)


# selecting our variables of interest and mutating on the redundant variables
team_batting2 <- Teams %>%
  select(yearID, teamID, G,  W,  L, R,  AB,  H,  X2B, X3B,  HR,  BB,  SO, SB, CS,  HBP, SF) %>%
  mutate(X1B = H - (X2B + X3B + HR),    
         X1Bn = X1B/AB, 
         X2Bn = X2B/AB, 
         X3Bn = X3B/AB, 
         XHRn = HR/AB, 
         XBBn = BB/AB)                        


# fitting a model with the useful statistics and a model with both useful and redundant statistics
fit1 <- lm(R ~ X1B + X2B + X3B + HR + BB + AB, data = team_batting2)
fit2 <- lm(R ~ X1B + X2B + X3B + HR + BB + X1Bn + X2Bn + X3Bn + XHRn + XBBn, data = team_batting2)


# summarizing these models
fit1_summary <- summary(fit1)
fit2_summary <- summary(fit2)


# printing the R^2 values
cat('R^2 \n')
fit1_summary$r.squared
fit2_summary$r.squared   # the more complex model always has as higher R^2


# printing the adjusted R^2 values
cat("\n Adjusted R^2 \n")
fit1_summary$adj.r.squared
fit2_summary$adj.r.squared  # the more complex model has as higher adjusted R^2


# printing the AIC values
cat("\n AIC \n")
AIC(fit1)
AIC(fit2)  # model 2 has as lower AIC so it is preferred


# printing the BIC values
cat("\n BIC \n")
BIC(fit1)
BIC(fit2)  # model 2 has as lower BIC so it is preferred


```






$\\$







#### Part 1.2: Cross-validation


Let's now compare the models using cross-validation. To keep things simple, we are just going to split the data once into a training set and a test set rather than do k-fold cross-validation. 

We will compare the models based on their mean squared prediction error (MSPE) where a smaller MSPE is better. 



```{r}


# create the training set and the test set
total_num_points <- dim(team_batting2)[1]
num_training_points <- floor(total_num_points/2)

training_data <- team_batting2[1:num_training_points, ]
test_data <- team_batting2[(num_training_points + 1):total_num_points, ]



# fit both models on the training data, and calculate the MSPE based on thier predictions on the test set

fit_cv_1 <- lm(R ~ X1B + X2B + X3B + HR + BB + AB, data = training_data)
test_predictions_1 <- predict(fit_cv_1, newdata = test_data)
MSPE_smaller_model <- mean((test_data$R - test_predictions_1)^2)
  

fit_cv_2 <- lm(R ~ X1B + X2B + X3B + HR + BB + X1Bn + X2Bn + X3Bn + XHRn + XBBn, data = training_data)
test_predictions_2 <- predict(fit_cv_2, newdata = test_data)
MSPE_larger_model <- mean((test_data$R - test_predictions_2)^2)


MSPE_smaller_model   # smaller MSPE is better
MSPE_larger_model



```






