---
title: "Class 21 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$



<!--  Please run the code in the  R chunk below once. This will install some packages and download data and images needed for these exercises.  -->

```{r message=FALSE, warning = FALSE, echo = FALSE, eval = FALSE}

# makes sure you have all the packages we have used in class installed
# SDS230::update_installed_packages()


SDS230::download_data("IPED_salaries_2016.rda")

SDS230::download_data("x_y_join.rda")


```




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)
library(dplyr)
library(ggplot2)

#options(scipen=999)


knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```



$\\$




## Overview

 * Logistic regression
 * Visualizing multiple regression models with ggplot
 * Joining data frames



$\\$




## Part 1: Logistic regression


Let us fit a logistic regression model that gives the probability that a professor is a Full professor (compared to an Assistant Professor) based on the professor's salary; i.e., the model is giving P(Full professor | salary). 

Note: In the IPED data, the cases are actually individual universities, and the salaries are averages over all professors of a particular rank, so what the model is really doing is giving P(Full professor rank | average salary) which is a little different than the variability that would be seen at the individual professor level. 

Let's start by loading the data, doing a little data cleaning, and plotting the data.



```{r}


# load the data into R
load("IPED_salaries_2016.rda")


# get assistant and full professor salaries 
assistant_full_data <- IPED_salaries %>%
  filter(endowment > 0, rank_name %in% c("Assistant", "Full")) %>%
  select(school, salary_tot, endowment, enroll_total, rank_name) %>%
  na.omit() %>%
  mutate(log_endowment = log10(endowment)) %>%
  mutate(rank_name = relevel(rank_name, "Assistant"))


# visualize the data
assistant_full_data  %>% 
  ggplot(aes(x = salary_tot, y = rank_name)) + 
  geom_jitter(alpha = .1, position = position_jitter(height = .2)) + 
  xlab("Salary ($)") +
  ylab("Faculty rank")

  
```





$\\$





#### Part 1.1.: Fitting a logistic model to the data


Let's now fit a logistic regression model using the `glm()` function. To fit a logistic regression model, we will use the family = "binomial" argument. What this function is doing is finding the "maximum likelihood" values for $\hat{\beta}_0$ and $\hat{\beta}_1$.

We can use the coefficients from the model to build a function that can give the probability of that the faculty rank is Full professor based on the salary value:

$P(\text{Full professor} | \text{salary}) = \frac{e^{\hat{\beta}_0 + \hat{\beta}_1 \cdot \text{salary}}}{1 + e^{\hat{\beta}_0 + \hat{\beta}_1 \cdot \text{salary}}}$



```{r}


# build the logistic regression function 
lr_fit <- glm(rank_name ~ salary_tot, data = assistant_full_data, family = "binomial")


# extract the coefficients
b0 <- coefficients(lr_fit)[1]
b1 <- coefficients(lr_fit)[2]


# create the prediction function 
get_full_prof_probability <- function(salary) {
  prob  <- (exp(b0 + b1 * salary)) / (1 + exp(b0 + b1 * salary))
  names(prob) <- NULL
  return(prob)
}
  

# what is the probability that if a school is paying $80,000 on average to a 
# particular faculty rank, this rank corresponds to the Full professor rank?
# (instead of the Assistant Professor rank)? 
get_full_prof_probability(80000)


```





$\\$





#### Part 1.2.: Plotting the logistic regression function 


Let's now plot the fitted logistic regression function to see how the probability that the rank corresponds to Full professor changes as a function of the salary amount. 



```{r, warning = FALSE}



# to plot this function we add 1 to it
plot_full_prof_probability <- function(salary) {
  get_full_prof_probability(salary) + 1
}


# plot the logistic regression function  
assistant_full_data  %>% 
  ggplot(aes(x = salary_tot, y = rank_name)) + 
  geom_jitter(alpha = .1, position = position_jitter(height = .2)) + 
  xlab("Salary ($)")  + 
  ylab ("Pr( Full prof | salary)") + 
  stat_function(fun = plot_full_prof_probability, color = "red") +
  xlim(-30000, 200000)  + 
  geom_vline(xintercept = 80000, col = "blue") + 
  geom_hline(yintercept = plot_full_prof_probability(80000), col = "blue", linetype="dotted") 


```



$\\$






#### Part 1.3: Multiple logistic regression


We can use multiple predictors in our logistic regression function as well. Let's create a function that gives the probability that the rank corresponds to Full professor given a salary amount a log10 endowment size.



```{r}


# fit the model
lr_fit2 <- glm(rank_name ~ salary_tot + log_endowment, data = assistant_full_data, family = "binomial")

# extract the regression coefficients
b0 <- coefficients(lr_fit2)[1]
b1 <- coefficients(lr_fit2)[2]
b2 <- coefficients(lr_fit2)[3]

# build a function that can predict faculty rank based on salary and endowment
get_full_prof_probability2 <- function(salary, log_endowment)  {
    (exp(b0 + (b1 * salary) + (b2 * log_endowment))) / (1 + exp(b0 + (b1 * salary) + (b2 * log_endowment)))
}


# predict probability Full professor if average salary is $80,000 
# and the school has a $10 million endowment ($10^7)
get_full_prof_probability2(80000, 7)

# predict probability Full professor if average salary is $80,000 
# and the school has a $10 billion endowment ($10^10)
get_full_prof_probability2(80000, 10)



```





$\\$






#### Part 1.4: Visualzing the multiple logistic regression function


Let's now visualize the probability function P(Full professor | salary, log_endowment)!



```{r}


# create a 2D plot of the probability that a car is new as a function of price and mileage
salary_intervals <- seq(10000, 120000, by = 100)
log_endowment_intervals <- seq(5, 10, by = .1)



# there are more efficient ways to do this using apply or mapping functions
salary_endowment_df <- data.frame()
for (currSalary in salary_intervals) {
  curr_df <- data.frame(salary = currSalary, 
                        log_endowment = log_endowment_intervals, 
                        prob_full_prof  = get_full_prof_probability2(currSalary,
                                                                     log_endowment_intervals))
  salary_endowment_df <- rbind(salary_endowment_df, curr_df)
}



# create the plot
salary_endowment_df %>%
  ggplot(aes(salary, log_endowment)) +
  geom_raster(aes(fill = prob_full_prof)) + 
  scale_fill_gradient(low = "black", high = "red") +
  xlab("Salary ($)") + 
  ylab("Log endowment") + 
  theme_classic()


```




$\\$





## Part 2: Visualizing multiple regression models with ggplot

In prior classes we have visualized multiple regression models with categorical predictors using base R graphics. In particular, we created scatter plots for data in different categories using the `plot()` and `points()` and used the `col` argument to color the points. We then added on regression lines using the `abline()` function. This method was useful for educational purposes so that we could see the connection between the model parameters that were estimated using the `lm()` function, and the underlying data. However, if we want to create better looking visualizations in a more efficient manner, then it is better to use ggplot.

Let's now use ggplot to visualize a multiple regression model that has one quantitative and one categorical variable. In particular, let's recreate the plot from class 19 part 3.6 where we display faculty salaries as a function of log endowment separately for different faculty ranks. 



```{r}

# create the IPED data subset used in class 19 part 3
IPED_2 <- IPED_salaries %>%
filter(endowment > 0) %>%
mutate(log_endowment = log10(endowment)) %>%
filter(CARNEGIE %in% c(15, 31)) %>%
filter(rank_name %in% c("Assistant", "Associate", "Full"))
dim(IPED_2)


# using ggplot
IPED_2 %>%
  ggplot(aes(log_endowment, salary_tot, col = rank_name)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  xlab("Log endowment") + 
  ylab("Salary ($)")





```




$\\$





## Part 3: Joining data frames


Often data of interest is spread across multiple data frames that need to be joined together into a single data frame for further analyses. We will explore how to do this using dplyr. 


Let's look at a very simple data set to explore joining data frames. 


```{r}

library(dplyr)


load('x_y_join.rda')

x
y



```


$\\$



#### Part 3.1: Left join

Left joins keep all rows in the left table.  

Data from right table added when there is the key matches, otherwise NA as added. 

Try to do a left join of the data frames x and y using their keys.


```{r}

left_join(x, y, by = c("key_x" = "key_y"))


```





$\\$





#### Part 3.2: Right join


Right joins keep all rows in the right table.  

Data from left table added when there is the key matches, otherwise NA as added. 

Try to do a right join of the data frames x and y using their keys.


```{r}

right_join(x, y, by = c("key_x" = "key_y"))


```



$\\$



#### Part 3.3: Inner join


Inner joins only keep rows in which there are matches between the keys in both tables

Try to do an inner join of the data frames x and y using their keys.


```{r}


inner_join(x, y, by = c("key_x" = "key_y"))


```



$\\$



#### Part 3.4: Full join

Full joins keep all rows in both table.  

NAs are added where there are no matches.




```{r}

full_join(x, y, by = c("key_x" = "key_y"))


```



$\\$




#### Part 3.5a: Duplicate keys


Duplicate keys are useful if there is a one-to-many relationship (duplicates are usually in the left table). 

Let's look at two other tables that have duplicate keys



```{r}

x2
y2

nrow(x2)
nrow(y2)


```




$\\$





#### Part 3.5b: Duplicate keys


If both tables have duplicate keys you get all possible combinations (Cartesian product). This is almost always an error! Always check the output dimension after you join a table because even if there is not a syntax error you might not get the table you are expecting!


Try doing a left join on the data frames x2 and y2 using only their first keys (i.e., key1_x and key1_y). Save the joined data frame to an object called `x2_joined`. Note that `x2_joined` has more rows than the original `x2` data frame despite the fact that you did a left join!  This is due to duplicate keys in both x2 and y2. 

Usually a mistake was made when a data frame ends up having more rows after a left join. It is good to check how many rows a data frame has before and after a join to catch any possible errors. 



```{r}

# initial left data frame only has 3 rows
nrow(x2)


# left join when both the left and right tables have duplicate keys
(x2_joined <- left_join(x2, y2, by = c("key1_x" = "key1_y")))


# output now has more rows than the initial table
nrow(x2_joined)


```




$\\$




#### Part 3.5c: Duplicate keys



To deal with duplicate keys in both tables, we can join the tables **using multiple keys** in order to make sure that each row is uniquely specified.

Try doing a left join on the data frames x2 and y2 using both the keys. Save the joined data frame to an object called `x2_joined_mult_keys`. Note that `x2_joined_mult_keys` has the same number of rows as the original `x2` data frame which is usually what we want when we do a left join.




```{r}


# initial left data frame only has 3 rows
nrow(x2)


# join the data frame using multiple keys
x2_joined_mult_keys <- left_join(x2, y2, c("key1_x" = "key1_y", "key2_x" = "key2_y"))

# output now only has 3 rows
nrow(x2_joined_mult_keys)


```



$\\$ 







