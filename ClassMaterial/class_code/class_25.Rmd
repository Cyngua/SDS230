---
title: "Class 25 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$



<!--  Please run the code in the  R chunk below once. This will install some packages and download data and images needed for these exercises.  -->

```{r message=FALSE, warning = FALSE, echo = FALSE, eval = FALSE}

# makes sure you have all the packages we have used in class installed
SDS230::update_installed_packages()




```




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)
library(dplyr)
library(ggplot2)
library(tidyr)
library(plotly)

#options(scipen=999)


knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```





$\\$





## Overview

 * Principal component analysis (PCA)
 * Clustering



$\\$






## Part 1: Principal component analysis (PCA)


Principal component analysis (PCA) is a method that can be used to reduce the dimensionality of a data set that has many quantitative variables; i.e., it can to reduce the number of columns that describe cases in a data frame. This can be useful for understanding and visualizing major trends in the data by reducing many variables to a few variables that capture most of the variability in the data. 

To see how to use PCA, let's look at personality traits of fictional characters. In particular, the 
Open-Source Psychometrics Project created an online “Which Character” personality quiz, where 800 fictional characters on 265 personality traits. These personality traits included traits such playful vs.serious, and charming vs. awkward (30 of these traits were emojis which we will ignore, so we will only deal with the remaining 235 traits). The analysis is based on a blog post by Alex Cookson although the methods described here are a bit different than in the blog post (the original blog post can be see at: https://www.alexcookson.com/post/2020-11-19-applying-pca-to-fictional-character-personalities/). 





$\\$






##### Part 1.1: Extract the data

Let's start by loading and processing the data. In particular we will: 

1. Load the data from the internet.

2. Create a data frame called `character_list` that has information on the 800 fictional characters.

3. Create a data frame called `spectrum_list` that has information on the 235 personality traits.

4. Create a data frame called `pca_data` that has the personality trait ratings in a "wide format".


For more information about converting data between long and wide formats using the `tidyr` package, see the videos lectures for class 15 (these types of transformations could also be very useful on your final project). 



```{r}


# the website where the data is located
data_name <- "https://raw.githubusercontent.com/tacookson/data/master/fictional-character-personalities/personalities.txt"

# read in the data
personalities <- read.csv(data_name, sep = "\t")


# extract information about the 800 characters in the data set



# extract information about the 235 personality traits that were rated




# get only the variables that are needed and put data into a wide format needed for PCA





```




$\\$





##### Part 1.2: Run PCA


Now that we have the data in an appropriate format we can run PCA. There are several different ways to run PCA in R including: 

* Using the `eigen()` on a data covariance matrix.
* Using the base R `prcomp()` function.
* Using the `psych` package.
* Using the `tidymodels`.


For the analyses below we will use the base R `prcomp()` function. We can pass the function the `pca_data`  we created above, and we will also set the `scale` argument to TRUE, which will make it so that data that has larger scales does not dominate the PCA decomposition (this argument should almost always be set to TRUE). We will save the results to an object called `prin_comp`. 



```{r}

# use the prcomp() to get the principal component loadings




```





$\\$





##### Part 1.3: Visualizing how many PCs are needed to capture most of the variation in the data


Let's now examine how much of the variation each PC accounts for. In particular, we want to assess whether we can describe most of the variability using just a few dimensions or if many dimensions are needed to describe the data. We can do this by creating a scree plot that plots the proportion of the variance that each PC accounts for. 


**Question**: How many principal components are needed to capture most of the variability in the data? 



```{r}


# create a scree plot








```




$\\$





##### Part 1.4: Interpretation of what the PCs mean

We can try to understand what personality traits a PC is representing by looking at the which personality traits have the highest loading values for a given PC. Let's focus on the first two PCs and see what personality traits they are capturing.



```{r}


# get the loadings for all PCs and combine this with the trait names




# Let's look at PC1




# Let's look at PC2





```





$\\$






##### Part 1.5: get the data scores

Now that we have the PCA *loadings*, we can created the *scores* for each of the 800 characters in our data frame. If we use fewer PCs than the 235 dimensions that are in the original data, this will create a representation that is lower dimension (i.e., we will have done dimensionality reduction). 

We can get the scores for our original data using the `predict()` function on our `prin_comp` object. We will also concatenate on the character names that are stored in the `character_list` data frame we created above so that we know the name that the scores correspond to. 


```{r}


# Project the data onto PCA loadings to get the PCA scores (the zi's)
# Also add on the name of a character that each row of scores corresponds to





```




$\\$








##### Part 1.6: Visualize and explore the data

Now that we have the PCA scores we can visualize the relationships between different characters in a 2-dimensional space, which is a much lower dimension than the original 235 dimensional space. 

Rather than visualize characters from all different sources, I am going to focus on sources I have some familiarity with (e.g., old movies and tv shows). You can examine different pieces of work that you are familiar with or look at all characters in the data set. 


```{r}


my_shows <- c("Fight Club", "Community",  
              "Game of Thrones", "Silicon Valley",
              "Star Trek: The Next Generation",  "Star Wars", "The Dark Knight", 
               "The Simpsons")

             







```





$\\$











## Part 2: Clustering


Let's now look at clustering methods that attempt to group together similar cases. We will start by looking at k-means clustering and then examine hierarchical clustering.



$\\$



#### Part 2.1.: k-means clustering - selecting the number of clusters


When running k-means clustering we need to select the number of clusters *k* before we run the clustering method. To determine how many clusters to use (i.e., which value of *k* to use) we can run the method on a range of values for *k* and visualize how the *within groups sum of squares* declines as a function of *k*. From looking at this graph, we can then select the value of *k* where the within groups sum of squares stops sharply declining.




```{r}


# scale the data to have mean of 0, sd of 1 



# Assess how many clusters to use





```





$\\$







#### Part 2.2.: k-means clustering - visualizing the clusters


One we have selected a value of *k* for the number of clusters we will use, we can run k-means clustering and then visualize which cases fall into which clusters. 



```{r}


# select the number of clusters and fit k-means




# create a data frame with the character names and which cluster they belong to




# visualize the data







```




$\\$





#### Part 2.3: Hierarchical clustering


We can also do hierarchical clustering where we successive add points on to existing clusters. This creates a tree-like structure that we can visualize to see which points are most similiar and how many clusters there are in the data. 


```{r, fig.width=15, fig.height=10}


# Get just the shows I am familiar with





# run the hierarchical clustering




# plot a dendrogram





```



$\\$




#### Part 2.4: Hierarchical clustering 2



We can also cut our hierarchical clustering at a particular level to get the desired number of clusters. 



```{r}

# create the appropriate data frame





# visualize the data





```



$\\$


