---
title: "Class 23 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$



<!--  Please run the code in the  R chunk below once. This will install some packages and download data and images needed for these exercises.  -->

```{r message=FALSE, warning = FALSE, echo = FALSE, eval = FALSE}

# makes sure you have all the packages we have used in class installed
#SDS230::update_installed_packages()

# SDS230::download_data("downloading.csv")


SDS230::download_data("IPED_salaries_2016.rda")


```




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)
library(dplyr)
library(ggplot2)

#options(scipen=999)


knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```



$\\$




## Overview

 * One-way ANOVA
 * Connections between ANOVA and linear regression
 * Pairwise comparisons after running an ANOVA



$\\$






## Part 1: One-way ANOVA 


Let's run a one-way ANOVA to see if there is a difference in how much faculty of different ranks get paid. 




$\\$





### Part 1.1: State the null and alternative hypotheses 


Let's start as always by stating the null and alternative hypotheses:

$H_0: \mu_{Assistant} = \mu_{Associate} = \mu_{Full}$
$H_A: \mu_{i} \ne \mu_{j}$ for some pair of i, j


$\alpha = 0.05$




$\\$





### Part 1.2a: Plot the data


```{r download_plot}

# get the data
load("IPED_salaries_2016.rda")


# create a boxplot of the data using ggplot
# get assistant and full professor salaries 
prof_data <- IPED_salaries %>%
  filter(endowment > 0, rank_name %in% c("Assistant", "Associate", "Full")) %>%
  select(school, salary_tot, endowment, enroll_total, rank_name) %>%
  na.omit()


# create a boxplot of the data using ggplot
prof_data %>%
  ggplot(aes(rank_name, salary_tot, fill = rank_name)) + 
  geom_boxplot() + 
  xlab("") + 
  ylab("Salary ($)")


```




```{r}


prof_summary <- prof_data %>%
  group_by(rank_name) %>%
  summarize(salary_tot = mean(salary_tot),
            sd_salary = sd(salary_tot))

prof_summary


prof_data %>%
  ggplot(aes(rank_name, salary_tot, col = rank_name)) + 
  geom_jitter(position = position_jitter(width = .2), alpha = .1) + 
  xlab("") + 
  ylab("Salary ($)") + 
  geom_crossbar(data = prof_summary, mapping = aes(ymin = salary_tot, ymax = salary_tot, col = rank_name), size=.5,width = .5)


```





$\\$







### Part 1.2b: Calculate the observed statistic

Our observed statistic is an F-statistic:

$F = \frac{\frac{1}{K-1}\sum_{i=1}^K n_i(\bar{x}_i - \bar{x}_{tot})^2}{\frac{1}{N-K}\sum_{i=1}^K \sum_{j=1}^{n_i} (x_{ij} - \bar{x}_i)^2}$


We will cheat and use the `lm()` and `anova()` functions to get the F-statistic. On the homework you will need to calculate this statistic from the data using dplyr!


```{r download_obs_stat}


# Getting the observed F-statistic for the faculty rank data using built in R functions
# On the homework be sure to use dplyr to actually calculate this statistic!

(obs_stat <- anova(lm(salary_tot ~ rank_name, data = prof_data))$F[1])


  
```






$\\$






### Step 3: Create the null distribution


Let's visualize the null distribution.


```{r download_null_dist, message = FALSE}

N <- nrow(prof_data)
K <- 3

(df1 <- K - 1)
(df2 <- N - K)

x <- seq(-.5, 5, by = 0.01)
y <- df(x, df1, df2)

plot(x, y, type = "l")



```




$\\$




### Part 1.4: Calculate a p-value


```{r download_pvalue}


# calculate the p-value
(p_value <- pf(obs_stat, df1, df2, lower.tail = FALSE))


```





$\\$






### Step 1.5: Make a decision


Since `r p_value` is less than $\alpha = 0.05$ we can reject the null hypothesis, although we should really check the model assumptions before making this claim. We will do this next...






$\\$









## Part 2: Connections between ANOVA and regression 


Let's look at connections between the least squares fit we used when fitting linear regression models and our one-way ANOVA. 




$\\$






### Step 2.1: Least squares offsets are the group means


Let's look at the mean download times for each time of day and compare it to the least squares offsets that the `lm()` function finds. 


```{r}

# get the mean and sd of the download times for each time of day
prof_summary <- prof_data %>%
  group_by(rank_name) %>%
  summarize(mean_salary = mean(salary_tot),
            sd_salary = sd(salary_tot))


# fit a linear model 
fit_prof <- lm(salary_tot ~ rank_name, data = prof_data)



# check that the least squares fit offsets are the means of each group
(summary_download <- summary(fit_prof))

fit_coefs <- coef(fit_prof)


c(fit_coefs[1], fit_coefs[1] + fit_coefs[2], fit_coefs[1] + fit_coefs[3])

prof_summary$mean_salary

```



$\\$




### Step 2.2: Using R to get an ANOVA table and to look at diagnostic plots


We can use the `anova()` function to create an ANOVA table, and we can use the `plot()` function to look at diagnostic plots to make sure our ANOVA conditions have been met. 



```{r}

# an easy way to get the ANOVA table using the ANOVA function 
(anova_table <- anova(fit_prof))

# we can see that the SST = SSG + SSE
(SST <- sum((prof_data$salary_tot - mean(prof_data$salary_tot))^2))

sum(anova_table$`Sum Sq`)



# we can use regression diagnostic plots to assess if ANOVA conditions have been met
par(mfrow = c(2, 2))
plot(fit_prof)


# we should also check that the maximum and minimum standard deviations are not greater 
# than a factor of 2 apart
print("sds")
prof_summary$sd_salary
max(prof_summary$sd_salary)/min(prof_summary$sd_salary)


```




$\\$






###  Part 2.3 Kruskal–Wallis test to see if any of our groups stochastically dominate another group. 

If we are concerned that our one-way ANOVA conditions are not met, we can run a Kruskal–Wallis test which does not rely on the assumptions of normality and homoscedasticity. We could also run a permutation test which does not rely on these assumptions either.


```{r}

#Kruskal–Wallis test
kruskal.test(salary_tot ~ rank_name, data = prof_data)


# compare to the ANOVA 
# anova(fit_prof)


```





$\\$






## Part 3: Pairwise comparisons after running a one-way ANOVA





###  Part 3.1 Pairwise comparisons in R

If we run a one-way ANOVA and the results are statistically significant, there are a number of tests we can run to see which pairs of results are significantly different. 



```{r}


# test with no multiple comparisons adjustment (not great)
(pair_tests <- pairwise.t.test(prof_data$salary_tot, prof_data$rank_name, p.adj = "none"))


# with the Bonferroni correction
pairwise.t.test(prof_data$salary_tot, prof_data$rank_name, p.adj = "bonferroni")


# Note, the Bonferroni p-values are 3 times larger than the p-values with no adjustment
3 * pair_tests$p.value


# Tukey's HSD test using the TukeyHSD() function 
fit_download_aov <- aov(salary_tot ~ rank_name, data = prof_data)
TukeyHSD(fit_download_aov)


```






