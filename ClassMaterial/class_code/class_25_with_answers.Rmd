---
title: "Class 25 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$



<!--  Please run the code in the  R chunk below once. This will install some packages and download data and images needed for these exercises.  -->

```{r message=FALSE, warning = FALSE, echo = FALSE, eval = FALSE}

# makes sure you have all the packages we have used in class installed
# SDS230::update_installed_packages()

SDS230::download_data("freshman-15.txt")

SDS230::download_data("IPED_salaries_2016.rda")


```




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)
library(dplyr)
library(ggplot2)
library(tidyr)
library(plotly)

#options(scipen=999)


knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```





$\\$





## Overview

 * A few additional topics on the ANOVA
 * Principal component analysis (PCA)
 * Clustering



$\\$



## Part 1: A few additional topics on the ANOVA



Let's briefly discuss a few last topics on the analysis of variance. 




### Part 1.1: Examening unbalanced data


In an unbalanced data, there are different numbers of measured responses at the different variable levels. When running an ANOVA on unbalanced data, one needs to be careful because there are different ways to calculate the sum of squares for the different factors, and this can lead to different results about which factors are statistically significant. Let's examine this using the IPED faculty salary data. 


```{r}

load("IPED_salaries_2016.rda")

# Factor A: lecturer, assistant, associate, full professor
# Factor B: liberal arts vs research university 

IPED_3 <- IPED_salaries %>%
  filter(rank_name %in% c("Lecturer", "Assistant", "Associate", "Full")) %>%
  mutate(rank_name  = droplevels(rank_name)) %>%
  filter(CARNEGIE %in% c(15, 31)) %>%
 # na.omit() %>%
  mutate(Inst_type = dplyr::recode(CARNEGIE, "31" = "Liberal arts", "15" = "Research extensive"))


# examine properties of the data 
table(IPED_3$Inst_type, IPED_3$rank_name)


```







$\\$







### Part 1.2: Examening unbalanced data - Type I sum of squares

In type I sum of squares, the sum of squares are calculated sequentially, where first SSA is taken into account, and then SSB is consider. In particular: 

* Factor A sum of squares is: SS(A) from fitting lm(y ~ A)
* Factor B sum of squares is: SS(B|A) from fitting lm(y ~ A + B) and subtracting this from SS(A)
* The interaction AB sum of squares is: SS(A, B, AB) - SS(A, B); i.e., the model is fit will lm(y ~ A*B) and then SS(A, B) is subtracted. 


```{r}

# Create a main effects and interaction model
fit_profs1 <- lm(salary_tot ~ Inst_type * rank_name, data = IPED_3)
fit_profs2 <- lm(salary_tot ~ rank_name * Inst_type, data = IPED_3)

anova(fit_profs1)
anova(fit_profs2)

```





$\\$




### Part 1.3: Examening unbalanced data - Type III sum of squares


In type III sum of squares, the sum of squares the full model is fit SS(A, B, AB) and then the sum of squares for each factor is determined by taking the full model SS(A, B, AB) and subtracting out the fit when a given factor is missing.

* Factor A sum of squares is: SS(A, B, AB) - SS(B, AB)
* Factor B sum of squares is: SS(A, B, AB) - SS(A, AB)
* The interaction AB sum of squares is: SS(A, B, AB) - SS(A, B)


```{r}

# type III sum of squares the order that variables are added does not matter
car::Anova(fit_profs1, type = "III")
car::Anova(fit_profs2, type = "III")

```




$\\$






### Part 1.4: Repeated measures ANOVA


In a repeated measures ANOVA, the same case/observational units are measured at each factor level. For example, we might want to understand if people prefer chocolate, butterscotch or caramel sauce on their ice cream. Rather than doing a "between subjects" experiment, where we would have different people taste ice cream with chocolate, butterscotch or caramel sauce, instead we can use a "within subjects" design where the each person in the experiment tastes and gives ratings for all three toppings. 

To run a repeated measures ANOVA, one gives each observational unit a unique ID, and then one treats this ID as another factor in the analysis; i.e., one runs a factorial analysis where one of the factors is the observational unit ID. 

An advantage of repeated measures ANOVA is similar to the advantage to running a paired samples t-test, namely it can reduce a lot of the between observational unit variability making it easier to see effects that are present. In fact, running a repeated measures ANOVA with a factor that only has two levels is equivalent to running a paired samples t-test. Let's explore this using the example of Freshman gaining weight from homework 4. 


```{r}

# load the data
freshman <- read.table("freshman-15.txt", header = TRUE) %>%
  mutate(Subject = as.factor(Subject))


# run a paired t-test testing H0: mu_diff = 0 vs. HA: mu_diff > 0, 
#   where mu_diff = mu_end_i - mu_start_i 

t.test(freshman$Terminal.Weight, freshman$Initial.Weight, paired = TRUE)



# let's transform to put it in a long format
freshman_long <- tidyr::pivot_longer(freshman, 
                                     cols = c("Initial.Weight", "Terminal.Weight"),
                                     names_to = "time_period", 
                                     values_to = "weight")


# let's run run a repeated measures ANOVA 
# we have the same p-value, and F = t^2
summary(aov(weight ~ time_period + Subject, data = freshman_long))




```



If you want more practice running a repeated measures ANOVA, you can analyze the popout attention data from homework 10, where you treat the participants in the study as a factor in the analysis. 





$\\$










## Part 2: Principal component analysis (PCA)


Principal component analysis (PCA) is a method that can be used to reduce the dimensionality of a data set that has many quantitative variables; i.e., it can to reduce the number of columns that describe cases in a data frame. This can be useful for understanding and visualizing major trends in the data by reducing many variables to a few variables that capture most of the variability in the data. 

To see how to use PCA, let's look at personality traits of fictional characters. In particular, the 
Open-Source Psychometrics Project created an online "Which Character" personality quiz, where 800 fictional characters on 265 personality traits. These personality traits included traits such playful vs.serious, and charming vs. awkward (30 of these traits were emojis which we will ignore, so we will only deal with the remaining 235 traits). The analysis is based on a blog post by Alex Cookson although the methods described here are a bit different than in the blog post (the original blog post can be see at: https://www.alexcookson.com/post/2020-11-19-applying-pca-to-fictional-character-personalities/). 





$\\$






##### Part 2.1: Extract the data

Let's start by loading and processing the data. In particular we will: 

1. Load the data from the internet.

2. Create a data frame called `character_list` that has information on the 800 fictional characters.

3. Create a data frame called `spectrum_list` that has information on the 235 personality traits.

4. Create a data frame called `pca_data` that has the personality trait ratings in a "wide format".


For more information about converting data between long and wide formats using the `tidyr` package, see the videos lectures for class 15 (these types of transformations could also be very useful on your final project). 



```{r}


# the website where the data is located
data_name <- "https://raw.githubusercontent.com/tacookson/data/master/fictional-character-personalities/personalities.txt"

# read in the data
personalities <- read.csv(data_name, sep = "\t")

# extract information about the 800 characters in the data set
character_list <- personalities %>%
  distinct(character_code, fictional_work, character_name)


# extract information about the 235 personality traits that were rated
spectrum_list <- personalities %>%
    filter(!is_emoji) %>%
  distinct(spectrum, spectrum_low, spectrum_high)


# get only the variables that are needed and put data into a wide format needed for PCA
pca_data <- personalities %>%
  filter(!is_emoji) %>%
  select(character_code, spectrum, mean) %>%
  pivot_wider(names_from = spectrum, values_from = mean) %>% 
  select(-character_code)

```




$\\$





##### Part 2.2: Run PCA


Now that we have the data in an appropriate format we can run PCA. There are several different ways to run PCA in R including: 

* Using the `eigen()` on a data covariance matrix.
* Using the base R `prcomp()` function.
* Using the `psych` package.
* Using the `tidymodels`.


For the analyses below we will use the base R `prcomp()` function. We can pass the function the `pca_data`  we created above, and we will also set the `scale` argument to TRUE, which will make it so that data that has larger scales does not dominate the PCA decomposition (this argument should almost always be set to TRUE). We will save the results to an object called `prin_comp`. 



```{r}

# use the prcomp() to get the principal component loadings
prin_comp <- prcomp(pca_data, scale = TRUE)

```





$\\$





##### Part 2.3: Visualizing how many PCs are needed to capture most of the variation in the data


Let's now examine how much of the variation each PC accounts for. In particular, we want to assess whether we can describe most of the variability using just a few dimensions or if many dimensions are needed to describe the data. We can do this by creating a scree plot that plots the proportion of the variance that each PC accounts for. 


**Question**: How many principal components are needed to capture most of the variability in the data? 



```{r}


plot(prin_comp$sdev^2/sum(prin_comp$sdev^2), 
     type = "o",
     ylab = "Proportion of the variance explained", 
     xlab = "PC number")


proportion_variance_explained <- prin_comp$sdev^2/sum(prin_comp$sdev^2)
sum(proportion_variance_explained[1:8])



plot(cumsum(prin_comp$sdev^2/sum(prin_comp$sdev^2)), 
     type = "o",
     ylab = "Proportion of the variance explained", 
     xlab = "PC number")


```




$\\$





##### Part 2.4: Interpretation of what the PCs mean

We can try to understand what personality traits a PC is representing by looking at the which personality traits have the highest loading values for a given PC. Let's focus on the first two PCs and see what personality traits they are capturing.


```{r}


# get the loadings for all PCs and combine this with the trait names
pc_loadings <- cbind(spectrum_list, as.data.frame(prin_comp$rotation)) %>%
  mutate(spectrum_low = as.character(spectrum_low),
         spectrum_high = as.character(spectrum_high))



# Let's look at PC1 - all negative so don't need to swap entries
pc1_traits <- pc_loadings %>%
  arrange(desc(abs(PC1))) %>%
  select(spectrum_low, spectrum_high, PC1) %>%
  head()



# Let's look at PC2
pc2_traits <- pc_loadings %>%
  arrange(desc(abs(PC2))) %>%
  select(spectrum_low, spectrum_high, PC2) %>%
  mutate(low_value = ifelse(PC2 < 0, spectrum_high, spectrum_low)) %>%
  mutate(high_value = ifelse(PC2 > 0, spectrum_high, spectrum_low)) %>%
  head()


pc1_traits
pc2_traits

```





$\\$






##### Part 2.5: get the data scores

Now that we have the PCA *loadings*, we can created the *scores* for each of the 800 characters in our data frame. If we use fewer PCs than the 235 dimensions that are in the original data, this will create a representation that is lower dimension (i.e., we will have done dimensionality reduction). 

We can get the scores for our original data using the `predict()` function on our `prin_comp` object. We will also concatenate on the character names that are stored in the `character_list` data frame we created above so that we know the name that the scores correspond to. 


```{r}


# Project the data onto PCA loadings to get the PCA scores (the ti's)
# Also add on the name of a character that each row of scores corresponds to
scores_df <-  cbind(character_list, as.data.frame(predict(prin_comp)))


```




$\\$








##### Part 2.6: Visualize and explore the data

Now that we have the PCA scores we can visualize the relationships between different characters in a 2-dimensional space, which is a much lower dimension than the original 235 dimensional space. 

Rather than visualize characters from all different sources, I am going to focus on sources I have some familiarity with (e.g., old movies and tv shows). You can examine different pieces of work that you are familiar with or look at all characters in the data set. 


```{r}


my_shows <- c("Fight Club", "Community",  
              "Seinfeld", "Silicon Valley",
              "Star Trek: The Next Generation",  "Star Wars", "The Dark Knight", 
               "The Simpsons")

             
g <- scores_df %>%
  filter(fictional_work %in% my_shows) %>%
  ggplot(aes(x = PC1, y = PC2, 
             name = character_name, 
             col = fictional_work)) + 
  geom_line() + 
  geom_point() 

#+ 
#  xlab(paste(pc1_traits$spectrum_high, pc1_traits$spectrum_low, sep = "-", collapse = "\n")) + 
#  ylab(paste(pc2_traits$low_value, pc2_traits$high_value, sep = "-", collapse = "\n"))


#ggplotly(g)

g


```





$\\$







## Part 3: Clustering


Let's now look at clustering methods that attempt to group together similar cases. We will start by looking at k-means clustering and then examine hierarchical clustering.



$\\$



#### Part 3.1: k-means clustering - selecting the number of clusters


When running k-means clustering we need to select the number of clusters *k* before we run the clustering method. To determine how many clusters to use (i.e., which value of *k* to use) we can run the method on a range of values for *k* and visualize how the *within groups sum of squares* declines as a function of *k*. From looking at this graph, we can then select the value of *k* where the within groups sum of squares stops sharply declining.




```{r}


# scale the data to have mean of 0, sd of 1 
data_scaled <- scale(pca_data)


# Assess how many clusters to use
within_cluster_ss <- NULL
for (k in 2:15) {
  curr_clusters <- kmeans(data_scaled, k)
  within_cluster_ss[k] <- curr_clusters$tot.withinss
}


plot(1:15, within_cluster_ss, type="o", xlab="Number of Clusters",
     ylab="Within groups sum of squares")


```





$\\$







#### Part 3.2: k-means clustering - visualizing the clusters


One we have selected a value of *k* for the number of clusters we will use, we can run k-means clustering and then visualize which cases fall into which clusters. 



```{r}


# select the number of clusters and fit kmeans
num_clusters_k <- 6

fit_clusters <- kmeans(data_scaled, num_clusters_k, nstart = 50)


# create a data frame with the character names and which cluster they belong to
cluster_df <- character_list %>%
  mutate(cluster = as.factor(fit_clusters$cluster)) 


# visualize the data
cluster_df %>%
  filter(fictional_work %in% my_shows) %>%
  group_by(cluster) %>%
  mutate(index_num = 1:n()) %>%
  ggplot(aes(cluster, index_num, label = character_name, col = fictional_work)) +
  geom_text(show.legend = FALSE, size = 3)


```




$\\$



#### Part 3.3: Hierarchical clustering


We can also do hierarchical clustering where we successive add points on to existing clusters. This creates a tree-like structure that we can visualize to see which points are most similar and how many clusters there are in the data. 


```{r, fig.width=15, fig.height=10}


# Get just the shows I am familiar with
data_scaled_my_shows <- cbind(character_list, data_scaled) %>%
  filter(fictional_work %in% my_shows)

rownames(data_scaled_my_shows) <- data_scaled_my_shows$character_name 

data_scaled_my_shows <- data_scaled_my_shows %>%
  select(-character_code, -character_name, -fictional_work)


# run the hierarchical clustering
hc_complete <- hclust(dist(data_scaled_my_shows), method = "complete")


# plot a dendrogram
plot(hc_complete)



```



$\\$




#### Part 3.4: Hierarchical clustering 2



We can also cut our hierarchical clustering at a particular level to get the desired number of clusters. 



```{r}

# create the appropriate data frame
hc_clusters_df <- cutree(hc_complete, 6) %>%
  as.data.frame() %>%
  add_rownames() %>%
  rename_with(~ c("character_name", "cluster")) %>%
  left_join(character_list) %>%
  group_by(cluster) %>%
  mutate(index_num = 1:n())


# visualize the data
hc_clusters_df %>%
  ggplot(aes(cluster, index_num, label = character_name, col = fictional_work)) +
  geom_text(show.legend = FALSE, size = 3)




```



$\\$



