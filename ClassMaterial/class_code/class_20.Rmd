---
title: "Class 20 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$




<!--  Please run the code in the  R chunk below once. This will install some packages and download data and images needed for these exercises.  -->

```{r message=FALSE, warning = FALSE, echo = FALSE, eval = FALSE}

# makes sure you have all the packages we have used in class installed
#SDS230::update_installed_packages()

SDS230::download_data("IPED_salaries_2016.rda")



```




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)
library(dplyr)
library(ggplot2)

#options(scipen=999)


knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```



$\\$




## Overview

 * Review of polynomial regression
 * Comparing models with R^2, adjusted R^2, AIC and BIC
 * Cross-validation
 * Visualizing multiple regression models with ggplot



$\\$





## Part 1: Review of polynomial regression


Last class we discussed polynomial regression where we created a model of the form $y = \hat{\beta}_0 + \hat{\beta}_1 \cdot x + \hat{\beta}_2 \cdot x^2 + ... + \hat{\beta}_k \cdot + x^k$. Let's explore polynomial regression a little bit more now.
 



$\\$





## Part 1.1: Comparing raw and orthogonal polynomials

As we discussed in class, by default the `poly()` function creates higher order polynomial terms that are *orthogonal* to each other. To better understand what *orthogonal* means you will need to take a class on linear algebra, but for our purposes, we can see that using orthogonal polynomials reduces multicolinearity. 

To explore this, let's create a synthetic data set where we will generate 1000 y values from a third degree polynomial function of x. In particular, we will use the equation: $y = 2x -3x^2 + 5x^3 + \epsilon$, where $\epsilon \sim ~ N(0, 20)$, We can then fit a 6 degree polynomial model to this data and see how well using regular or orthogonal polynomial linear regression models leads to identifying terms of all degrees that are in the real relationship between x and y. 


```{r}

set.seed(100)

# generate a synethetic data set
x <- rnorm(1000)
y <- 2*x - 3*x^2 + 5*x^3 + 20 * rnorm(1000)
plot(x, y)


# fit without orthogonal polynomials (raw = TRUE)




# fit using orthogonal polynomials (raw = FALSE)





```

Questions:
- Does the coefficient of determination $R^2$ differ when using orthogonal vs. raw polynomials? 
- Does one model better identify which terms are in the real x, y relationship? 




$\\$





## Part 1.2: Writing a function to compare R^2 statistics for different degree polynomial models


Let's now evaluate how well using polynomial models of different degrees fit the assistant professor faculty salary data. As we discussed in class, the $R^2$ statistic (coefficient of determination) tells us the proportion of variability in y that is explained by our model. To to assess how "good" a model is, we will fit polynomial models of different degrees and see how this affects our $R^2$ statistic. 

To make it easier repeat the process of fitting different degree polynomial models to the faculty salary data, let's write a function called `poly_fit_r2()` that will: 

a. take as an input argument `degree` that is the degree of a polynomial
b. will fit a polynomial model of based on the degree given using raw polynomials
c. will return the $R^2$ statistic

Then let's use this function to compare $R^2$ statistic for different degree fits.


```{r}

# get the assistant professor data
load("IPED_salaries_2016.rda")
assistant_data <- filter(IPED_salaries,  endowment > 0,  rank_name == "Assistant")  



# write a function that will fit a polynomial of a given degree and return the R^2 statistic










# assess the R^2 statistic for different degree models 





```


Which degree model has the highest $R^2$ statistic value? 





$\\$






## Part 1.3: Writing a function to visualize different degree models

Let's extend our function to take an additional Boolean argument called `plot_model` that when set to `TRUE` will create a scatter plot of the data and also plot the polynomial fit model as a red line. 

Once we have created this function let's visualize polynomial fits of different degrees.



```{r}


# create the function












# visual polynomial fits of different degrees






```


$\\$



What would Michael Jackson say about this?  

https://www.youtube.com/watch?v=DQWI1kvmwRg






$\\$








## Part 2:  Comparing models with R^2, adjusted R^2, AIC, BIC and cross-validation


We can compare models using several statistics including $R^2$, $R^2_{adj}$, $AIC$, $BIC$ and cross-validation. Let's try comparing models using these statistics to see how well each statistic captures our intuition about the best model to use. 





$\\$







#### Part 2.1: Comparing models with R^2, adjusted R^2, AIC, BIC


Let's compare models using these statistics: 

 * `R^2`: where a higher value means a better fit
 * `Adjusted R^2`: where higher value means a better fit
 * `AIC`: where a lower value means a better model
 * `BIC`: where a lower value means a better model

We can use either the synthetic x, y, data we created above, or the assistant professor data. We will fit the model with different degree polynomials and wee which modle each statistic selects. 


```{r}


library(dplyr)


# fit models of degree 1, 3, and 5





# summarizing these models






# printing the R^2 values (higher is better)
cat('R^2 \n')






# printing the adjusted R^2 values (higher is better)
cat("\n Adjusted R^2 \n")






# printing the AIC values (lower is better)
cat("\n AIC \n")



  

# printing the BIC values (lower is better)
cat("\n BIC \n")






```





$\\$







#### Part 2.2: Comparing models using cross-validation


Let's now compare the models using cross-validation. To keep things simple, we are just going to split the data once into a training set and a test set rather than do k-fold cross-validation. 

We will compare the models based on their mean squared prediction error (MSPE) where a smaller MSPE is better. 



```{r}


# create the training set and the test set






# fit both models on the training data, and calculate the MSPE based on their predictions on the test set









# use a for loop to compare the MSPE for models of degree 1 to 7









```






$\\$






## Part 3: Visualizing multiple regression models with ggplot

In prior classes we have visualized multiple regression models with categorical predictors using base R graphics. In particular, we created scatter plots for data in different categories using the `plot()` and `points()` and used the `col` argument to color the points. We then added on regression lines using the `abline()` function. This method was useful for educational purposes so that we could see the connection between the model parameters that were estimated using the `lm()` function, and the underlying data. However, if we want to create better looking visualizations in a more efficient manner, then it is better to use ggplot.

Let's now use ggplot to visualize a multiple regression model that has one quantitative and one categorical variable. In particular, let's recreate the plot from class 19 part 3.6 where we display faculty salaries as a function of log endowment separately for different faculty ranks. 


**NOTE: You are not allowed to use ggplot to plot model fits on homework 8, but instead you need to use base R graphics**



```{r}

library(ggplot2)

# create the IPED data subset used in class 19 part 3
IPED_2 <- IPED_salaries %>%
filter(endowment > 0) %>%
mutate(log_endowment = log10(endowment)) %>%
filter(CARNEGIE %in% c(15, 31)) %>%
filter(rank_name %in% c("Assistant", "Associate", "Full"))
dim(IPED_2)



# using ggplot








```

