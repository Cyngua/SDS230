---
title: "Class 19 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$



<!--  Please run the code in the  R chunk below once. This will install some packages and download data and images needed for these exercises.  -->

```{r message=FALSE, warning = FALSE, echo = FALSE, eval = FALSE}

# makes sure you have all the packages we have used in class installed
SDS230::update_installed_packages()


SDS230::download_data("IPED_salaries_2016.rda")


```




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)
library(dplyr)

#options(scipen=999)


knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```



$\\$




## Overview


 * Analysis of variance for regression
 * Multiple linear regression
 * Categorical predictors and interactions
 * Non-linear functions of the predictors




$\\$






## Part 1: Analysis of variance (ANOVA) for regression

The code below will create an ANOVA table for regression for a model predicting salary as a function of the log of a school's endowment. We will use data from assistant professors from the IPED data. 


```{r iped_anova}

# load the data into R
load("IPED_salaries_2016.rda")

assistant_data <- IPED_salaries %>%
  filter(endowment > 0, rank_name == "Assistant") %>%
  select(school, salary_tot, endowment, enroll_total) %>%
  na.omit() %>%
  mutate(log_endowment = log10(endowment))
  

lm_fit <- lm(salary_tot ~ log_endowment, data = assistant_data)

anova(lm_fit)

```





$\\$





## Part 2: Multiple linear regression on the faculty salary data


Let's now use the faculty salary data to explore multiple linear regression for building a model to predict faculty salary from the endowment size of a school and the number of students enrolled. 



#### Part 2.1: Exploring the enrollment data

In our previous analyses we have built models predicting faculty salaries based on the size of the school's endowment. Before we start on multiple regression, let's look at have faculty salaries are affected by the another variable, namely, number of students enrolled at a school. Before starting an analysis it is often worth thinking about our expectations. Here we might expect that schools that have higher enrollment numbers might be able to pay their faculty more since they have a higher revenue stream from the larger number of students paying tuition. Thus if we model $\text{salary} = \hat{\beta}_0 + \hat{\beta}_1 \cdot \text{enrollment}$ we might expect $\hat{\beta}_1$ to be positive. 

Let's start our exploratory analysis by plotting the relationship between faculty salary and the number of students enrolled. If the relationship does not appear linear, we can transform the variables (as is often done in the "choose" step of model building).


```{r}

# plot the relationship between salary and enrollment
plot(assistant_data$enroll_total, assistant_data$salary_tot)

```


$\\$


From looking at this plot we see that the there is a large range of enrollment values with a few large numbers. Thus it might be better to log transform the x-values (often when values can only be positive, taking a log transformation leads to more linear relationship). Let's mutate on a variable `log_enroll` to our assistant data frame and then plot the relationship between salary and `log_enroll`. We will use log10 here to be consistent with our transformation of endowment and also since it is easier for us to think in terms of "order of magnitude" for this problem.  


```{r}


# the relationship does not appear linear, let's mutate on log enrollment
assistant_data <- assistant_data %>%
  mutate(log_enroll = log10(enroll_total)) 


# plot the relationship between salary and log enrollment
plot(assistant_data$log_enroll, assistant_data$salary_tot)


```


**Question:** Does the relationship appear more linear now? 




$\\$





#### Part 2.2: Fittting a simple linear regression model for predicting salary as a function of log enrollment


Let's now fit a simple linear regression model $\text{salary} = \hat{\beta}_0 + \hat{\beta}_1 \cdot \text{log(enrollment)}$. Let's also plot the model and look at some inferential statistics by using the `summary()` function on our model.


```{r}

plot(assistant_data$log_enroll, assistant_data$salary_tot)
lm_fit_enroll <- lm(salary_tot ~ log_enroll, data = assistant_data)
abline(lm_fit_enroll, col = "red")

(summary_enroll <- summary(lm_fit_enroll))

```


$\\$




#### Part 2.3:  Comparing the simple linear regression enrollment and endowment models

Let's compare these models predicting salary from  enrollment vs. endowment in terms of which model can explain most of the variability in salaries in terms of the $r^2$ statistic. Let's also create a scatter plot of the relationships between these three variables using the `paris()` function.


```{r}


# compare r^2 and look at all scatter plots
lm_fit_endow <- lm(salary_tot ~ log_endowment, data = assistant_data)
summary_endow <- summary(lm_fit_endow)
#plot(assistant_data$log_endowment, assistant_data$salary_tot)
#abline(lm_fit_endow, col = "red")


paste("Endowment model r^2", round(summary_endow$r.squared, 4))
paste("Enrollment model r^2", round(summary_enroll$r.squared, 4))

pairs(select(assistant_data, salary_tot, log_endowment, log_enroll))



```



$\\$




#### Part 2.4: Multiple regression 


Let's now fit a multiple regression model for predicting salary using both endowment and enrollment as explanatory variables to see if using both these variables allows us to better predict salary than either variable along. In particular, we are fitting the model $\text{salary} = \hat{\beta}_0 + \hat{\beta}_1 \cdot \text{log(endowment)} + \hat{\beta}_1 \cdot \text{log(enrollment)}$.



```{r}


(lm_fit_endow_enroll <- lm(salary_tot ~ log_endowment + log_enroll, data = assistant_data))

summary(lm_fit_endow_enroll)


```


**Question:** Does this model account for more of the variability than the simple regression models we fit? 



$\\$




#### Part 2.5: Test for comparing nested models


When we have nested models, we can use an ANOVA test based on the F-statistic to assess if adding additional explanatory variables leads to a model that can account for more of the variability in a response variable.

A Model 1 is nested in a Model 2 if the parameters in Model 1 are a subset of the parameters in Model 2. Here, our model using only the endowment as the explanatory variable is nested within in the model that uses endowment and enrollment as explanatory variables. Let's uses the `anova()` function to test if adding the enrollment explanatory leads to a statistically significant increase in the amount of variability that can be accounted for.


```{r}

anova(lm_fit_endow, lm_fit_endow_enroll)

```




$\\$




## Part 3: Exploring categorical predictors and interactions


Let's now examine how much faculty salaries increase as a function of log endowment size taking into account the rank that different professors have. 



**Part 3.1 (2 points)**: Let's start by creating a data set called `IPED_2` which is modified `IPED_2` in the following way:

1. Only include data from institutions with a CARNEGIE classification of 15 or 31  (these correspond to R1 institutions and liberal arts colleges).

2. Only use the faculty ranks of Assistant, Associate, and Full professors



```{r}

IPED_2 <- IPED_salaries %>% 
  filter(endowment > 0) %>%
  mutate(log_endowment = log10(endowment)) %>%
  filter(CARNEGIE %in% c(15, 31)) %>%
  filter(rank_name %in% c("Assistant", "Associate", "Full")) 


dim(IPED_2)


```




$\\$




#### Part 3.2: Visualizing the data


Let's visualize the data by creating a scatter plot showing the total salary that faculty get paid (salary_tot) as a function of the log endowment size, where each faculty rank is in a different color using base R graphics.



```{r}


plot(salary_tot ~ log_endowment, data = filter(IPED_2, rank_name == "Assistant"), ylim = c(0, 250000))

points(salary_tot ~ log_endowment, data = filter(IPED_2, rank_name == "Associate"), col = 'red')

points(salary_tot ~ log_endowment, data = filter(IPED_2, rank_name == "Full"), col = 'blue')



```




$\\$





#### Part 3.3: Fitting a linear model to the data


Let's now fit a linear regression model for total salary as a function of log endowment size, but use a separate y-intercept for each of the 3 faculty ranks (and use the same slope for all ranks). We can then use the `summary()` function to extract information about the model.



```{r}

fit_prof_rank_offset <- lm(salary_tot ~ log_endowment + rank_name, data = IPED_2)

summary(fit_prof_rank_offset)

```

**Question:** 

How much of the total variability does the model explain? 

Are there differences between the Full Professors and the other ranks in terms of their intercepts? 

Also, how much less is an Assistant Professor making relative to a Full Professor?




$\\$





#### Part 3.4:  Visualizing the model fit

Let's recreate the scatter plot we created in part 2.2 using the same colors. Now, however, let's also add on the regression lines with different y-intercepts that you fit in part 2.4 (using the appropriate colors to match the colors of the points). 


```{r}


plot(salary_tot ~ log_endowment, data = filter(IPED_2, rank_name == "Assistant"), ylim = c(0, 250000))

points(salary_tot ~ log_endowment, data = filter(IPED_2, rank_name == "Associate"), col = 'red')

points(salary_tot ~ log_endowment, data = filter(IPED_2, rank_name == "Full"), col = 'blue')


the_coefs <- coef(fit_prof_rank_offset)

abline(the_coefs[1], the_coefs[2], col = "blue")
abline(the_coefs[1] + the_coefs[3], the_coefs[2], col = "red")
abline(the_coefs[1] + the_coefs[4], the_coefs[2], col = "black")


```


**Question** Does using the same slope but different offsets seem to be adequate for capturing the trends in the data? 





$\\$







#### Part 3.5: Fitting models with different intercepts and slopes


Now let's fit a linear regression model for total salary as a function of log endowment size, but use separate y-intercepts **and slopes** for each of the 3 faculty ranks. 



```{r}


fit_prof_rank <- lm(salary_tot ~ log_endowment + rank_name + log_endowment * rank_name, data = IPED_2)

summary(fit_prof_rank)


```



$\\$



**Question**: How much of the total sum of squares of faculty salary is this model capturing? 




$\\$







#### Part 3.6: Visualizing the model 


Now let's again recreate the scatter plot you created in part 2.2 using the same colors and let's add on the regression line with different y-intercepts and slopes based on the model you fit in part 2.5 (again use the appropriate colors).



```{r}


plot(salary_tot ~ log_endowment, data = filter(IPED_2, rank_name == "Assistant"), ylim = c(0, 250000))

points(salary_tot ~ log_endowment, data = filter(IPED_2, rank_name == "Associate"), col = 'red')

points(salary_tot ~ log_endowment, data = filter(IPED_2, rank_name == "Full"), col = 'blue')


the_coefs <- coef(fit_prof_rank)

abline(the_coefs[1], the_coefs[2], col = "blue")
abline(the_coefs[1] + the_coefs[3], the_coefs[2] + the_coefs[5], col = "red")
abline(the_coefs[1] + the_coefs[4], the_coefs[2] + the_coefs[6], col = "black")


```

**Question** Does this model seem like a better fit and do you think there are ways you could further improve on this model? 




$\\$





### Part 3.7 Comparing models


The model you fit in Part 2.5 is nested within the model you fit in Part 2.3. We can use the `anova()` function to compare such nested models. Does adding the additional slopes for each rank seem to improve the model fit? 



```{r}


anova(fit_prof_rank_offset, fit_prof_rank)


```



$\\$



## Part 4: Non-linear transformations of explanatory variables

We can also fit models that have non-linear transformations to our explanatory variables. This allows us to fit a much broader range of models to our data. In particular, we can fit models that are polynomial functions of our original explanatory variables: 

$y = \hat{\beta}_0 + \hat{\beta}_1 \cdot x + \hat{\beta}_2 \cdot x^2 + ... + \hat{\beta}_k \cdot + x^k$




$\\$




#### Part 4.1: General linear models 

Let's explore fitting a model that predicts salary from log endowment, log endowment squared and log endowment cubed using our salary data from assistant professors:

$\text{salary} = \hat{\beta}_0 + \hat{\beta}_1 \cdot \text{log(endowment)} + \hat{\beta}_2 \cdot \text{log(endowment)}^2 + \hat{\beta}_2 \cdot + \text{log(endowment)}^3$





```{r}

# refit our original degree 1 model
lm_fit_1 <- lm(salary_tot ~ log_endowment, data = assistant_data)
summary_1 <- summary(lm_fit_1)


# fit a cubic model
lm_fit_3 <- lm(salary_tot ~ log_endowment + 
                 I(log_endowment^2) + 
                 I(log_endowment^3), 
               data = assistant_data)

# get statistics on our cubic fit
(summary_3 <- summary(lm_fit_3))


# compare the r^2
summary_1$r.squared
summary_3$r.squared


```

**Question:** Are the higher order terms statistically significant? 





$\\$





#### Part 4.2: Let's visualize our model



```{r}


predict_df <- data.frame(log_endowment = seq(0, 11, by = .1))


plot(assistant_data$log_endowment, assistant_data$salary_tot)

points(predict_df$log_endowment, predict(lm_fit_3, newdata = predict_df), type = "l", col = "red")


```


**Question:** Does this model visually appear to fit the data better? 




$\\$





#### Part 4.3: Using an ANOVA to compare the nested models

Because our linear model is nested within our cubic model, we can use an ANOVA to evalualte whether our cubic model accounts for more of the variability than our linear model.


```{r}

anova(lm_fit_1, lm_fit_3)


```










