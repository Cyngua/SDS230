---
title: "Class 26 notes and code"
output:
  pdf_document: default
  html_document: default
---





$\\$



<!--  Please run the code in the  R chunk below once. This will install some packages and download data and images needed for these exercises.  -->

```{r message=FALSE, warning = FALSE, echo = FALSE, eval = FALSE}

# makes sure you have all the packages we have used in class installed
# SDS230::update_installed_packages()


SDS230::download_data("IPED_salaries_2016.rda")


```




```{r setup, include=FALSE}

# install.packages("latex2exp")

library(latex2exp)
library(dplyr)
library(ggplot2)
library(tidyr)
library(plotly)

#options(scipen=999)


knitr::opts_chunk$set(echo = TRUE)

set.seed(123)

```





$\\$





## Overview

 * Principal component analysis (PCA)
 * Clustering





$\\$






## Part 1: Principal component analysis (PCA)


Principal component analysis (PCA) is a method that can be used to reduce the dimensionality of a data set that has many quantitative variables; i.e., it can to reduce the number of columns that describe cases in a data frame. This can be useful for understanding and visualizing major trends in the data by reducing many variables to a few variables that capture most of the variability in the data. 

To see how to use PCA, let's look at personality traits of fictional characters. In particular, the 
Open-Source Psychometrics Project created an online "Which Character" personality quiz, where 800 fictional characters on 265 personality traits. These personality traits included traits such playful vs.serious, and charming vs. awkward (30 of these traits were emojis which we will ignore, so we will only deal with the remaining 235 traits). The analysis is based on a blog post by Alex Cookson although the methods described here are a bit different than in the blog post (the original blog post can be see at: https://www.alexcookson.com/post/2020-11-19-applying-pca-to-fictional-character-personalities/). 





$\\$






##### Part 1.1: Extract the data

Let's start by loading and processing the data. In particular we will: 

1. Load the data from the internet.

2. Create a data frame called `character_list` that has information on the 800 fictional characters.

3. Create a data frame called `spectrum_list` that has information on the 235 personality traits.

4. Create a data frame called `pca_data` that has the personality trait ratings in a "wide format".


For more information about converting data between long and wide formats using the `tidyr` package, see the videos lectures for class 15 (these types of transformations could also be very useful on your final project). 



```{r}


# the website where the data is located
data_name <- "https://raw.githubusercontent.com/tacookson/data/master/fictional-character-personalities/personalities.txt"

# read in the data
personalities <- read.csv(data_name, sep = "\t")

# extract information about the 800 characters in the data set
character_list <- personalities %>%
  distinct(character_code, fictional_work, character_name)


# extract information about the 235 personality traits that were rated
spectrum_list <- personalities %>%
    filter(!is_emoji) %>%
  distinct(spectrum, spectrum_low, spectrum_high)


# get only the variables that are needed and put data into a wide format needed for PCA
pca_data <- personalities %>%
  filter(!is_emoji) %>%
  select(character_code, spectrum, mean) %>%
  pivot_wider(names_from = spectrum, values_from = mean) %>% 
  select(-character_code)

```




$\\$





##### Part 1.2: Run PCA


Now that we have the data in an appropriate format we can run PCA. There are several different ways to run PCA in R including: 

* Using the `eigen()` on a data covariance matrix.
* Using the base R `prcomp()` function.
* Using the `psych` package.
* Using the `tidymodels`.


For the analyses below we will use the base R `prcomp()` function. We can pass the function the `pca_data`  we created above, and we will also set the `scale` argument to TRUE, which will make it so that data that has larger scales does not dominate the PCA decomposition (this argument should almost always be set to TRUE). We will save the results to an object called `prin_comp`. 



```{r}

# use the prcomp() to get the principal component loadings
prin_comp <- prcomp(pca_data, scale = TRUE)

```





$\\$





##### Part 1.3: Visualizing how many PCs are needed to capture most of the variation in the data


Let's now examine how much of the variation each PC accounts for. In particular, we want to assess whether we can describe most of the variability using just a few dimensions or if many dimensions are needed to describe the data. We can do this by creating a scree plot that plots the proportion of the variance that each PC accounts for. 


**Question**: How many principal components are needed to capture most of the variability in the data? 



```{r}


plot(prin_comp$sdev^2/sum(prin_comp$sdev^2), 
     type = "o",
     ylab = "Proportion of the variance explained", 
     xlab = "PC number")


proportion_variance_explained <- prin_comp$sdev^2/sum(prin_comp$sdev^2)
sum(proportion_variance_explained[1:8])



plot(cumsum(prin_comp$sdev^2/sum(prin_comp$sdev^2)), 
     type = "o",
     ylab = "Proportion of the variance explained", 
     xlab = "PC number")


```




$\\$





##### Part 1.4: Interpretation of what the PCs mean

We can try to understand what personality traits a PC is representing by looking at the which personality traits have the highest loading values for a given PC. Let's focus on the first two PCs and see what personality traits they are capturing.


```{r}


# get the loadings for all PCs and combine this with the trait names
pc_loadings <- cbind(spectrum_list, as.data.frame(prin_comp$rotation)) %>%
  mutate(spectrum_low = as.character(spectrum_low),
         spectrum_high = as.character(spectrum_high))



# Let's look at PC1 - all negative so don't need to swap entries
pc1_traits <- pc_loadings %>%
  arrange(desc(abs(PC1))) %>%
  select(spectrum_low, spectrum_high, PC1) %>%
  head()



# Let's look at PC2
pc2_traits <- pc_loadings %>%
  arrange(desc(abs(PC2))) %>%
  select(spectrum_low, spectrum_high, PC2) %>%
  mutate(low_value = ifelse(PC2 < 0, spectrum_high, spectrum_low)) %>%
  mutate(high_value = ifelse(PC2 > 0, spectrum_high, spectrum_low)) %>%
  head()


pc1_traits
pc2_traits

```





$\\$






##### Part 1.5: get the data scores

Now that we have the PCA *loadings*, we can created the *scores* for each of the 800 characters in our data frame. If we use fewer PCs than the 235 dimensions that are in the original data, this will create a representation that is lower dimension (i.e., we will have done dimensionality reduction). 

We can get the scores for our original data using the `predict()` function on our `prin_comp` object. We will also concatenate on the character names that are stored in the `character_list` data frame we created above so that we know the name that the scores correspond to. 


```{r}


# Project the data onto PCA loadings to get the PCA scores (the ti's)
# Also add on the name of a character that each row of scores corresponds to
scores_df <-  cbind(character_list, as.data.frame(predict(prin_comp)))


```




$\\$








##### Part 1.6: Visualize and explore the data

Now that we have the PCA scores we can visualize the relationships between different characters in a 2-dimensional space, which is a much lower dimension than the original 235 dimensional space. 

Rather than visualize characters from all different sources, I am going to focus on sources I have some familiarity with (e.g., old movies and tv shows). You can examine different pieces of work that you are familiar with or look at all characters in the data set. 


```{r}


my_shows <- c("Fight Club", "Community",  
              "Seinfeld", "Silicon Valley",
              "Star Trek: The Next Generation",  "Star Wars", "The Dark Knight", 
               "The Simpsons")

             
g <- scores_df %>%
  filter(fictional_work %in% my_shows) %>%
  ggplot(aes(x = PC1, y = PC2, 
             name = character_name, 
             col = fictional_work)) + 
  geom_line() + 
  geom_point() 

#+ 
#  xlab(paste(pc1_traits$spectrum_high, pc1_traits$spectrum_low, sep = "-", collapse = "\n")) + 
#  ylab(paste(pc2_traits$low_value, pc2_traits$high_value, sep = "-", collapse = "\n"))


#ggplotly(g)

g


```





$\\$







## Part 2: Clustering


Let's now look at clustering methods that attempt to group together similar cases. We will start by looking at k-means clustering and then examine hierarchical clustering.



$\\$



#### Part 2.1: k-means clustering - selecting the number of clusters


When running k-means clustering we need to select the number of clusters *k* before we run the clustering method. To determine how many clusters to use (i.e., which value of *k* to use) we can run the method on a range of values for *k* and visualize how the *within groups sum of squares* declines as a function of *k*. From looking at this graph, we can then select the value of *k* where the within groups sum of squares stops sharply declining.




```{r}


# scale the data to have mean of 0, sd of 1 
data_scaled <- scale(pca_data)


# Assess how many clusters to use
within_cluster_ss <- NULL
for (k in 2:15) {
  curr_clusters <- kmeans(data_scaled, k)
  within_cluster_ss[k] <- curr_clusters$tot.withinss
}


plot(1:15, within_cluster_ss, type="o", xlab="Number of Clusters",
     ylab="Within groups sum of squares")


```





$\\$







#### Part 2.2: k-means clustering - visualizing the clusters


One we have selected a value of *k* for the number of clusters we will use, we can run k-means clustering and then visualize which cases fall into which clusters. 



```{r}


# select the number of clusters and fit kmeans
num_clusters_k <- 6

fit_clusters <- kmeans(data_scaled, num_clusters_k, nstart = 50)


# create a data frame with the character names and which cluster they belong to
cluster_df <- character_list %>%
  mutate(cluster = as.factor(fit_clusters$cluster)) 


# visualize the data
cluster_df %>%
  filter(fictional_work %in% my_shows) %>%
  group_by(cluster) %>%
  mutate(index_num = 1:n()) %>%
  ggplot(aes(cluster, index_num, label = character_name, col = fictional_work)) +
  geom_text(show.legend = FALSE, size = 3)


```





$\\$





#### Part 2.3: Hierarchical clustering


We can also do hierarchical clustering where we successive add points on to existing clusters. This creates a tree-like structure that we can visualize to see which points are most similar and how many clusters there are in the data. 


```{r, fig.width=15, fig.height=10}


# Get just the shows I am familiar with
data_scaled_my_shows <- cbind(character_list, data_scaled) %>%
  filter(fictional_work %in% my_shows)

rownames(data_scaled_my_shows) <- data_scaled_my_shows$character_name 

data_scaled_my_shows <- data_scaled_my_shows %>%
  select(-character_code, -character_name, -fictional_work)


# run the hierarchical clustering
hc_complete <- hclust(dist(data_scaled_my_shows), method = "complete")


# plot a dendrogram
plot(hc_complete)



```



$\\$




#### Part 2.4: Hierarchical clustering 2



We can also cut our hierarchical clustering at a particular level to get the desired number of clusters. 



```{r}

# create the appropriate data frame
hc_clusters_df <- cutree(hc_complete, 6) %>%
  as.data.frame() %>%
  add_rownames() %>%
  rename_with(~ c("character_name", "cluster")) %>%
  left_join(character_list) %>%
  group_by(cluster) %>%
  mutate(index_num = 1:n())


# visualize the data
hc_clusters_df %>%
  ggplot(aes(cluster, index_num, label = character_name, col = fictional_work)) +
  geom_text(show.legend = FALSE, size = 3)




```



$\\$



